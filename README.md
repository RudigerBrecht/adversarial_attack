# adversarial_attack

The notebook contains the training of a classifier for the CIFAR-10 dataset. 
After training the classifier the fast gradient sign method is applied to the test dataset.
The  adversarial attack reduces the accuracy from 85% to 9%.
